# Loymax RAG QA Service

Прототип сервиса вопрос–ответ (Question Answering) на основе **Retrieval‑Augmented Generation (RAG)**.
Сервис индексирует документы (например, из датасета RuBQ 2.0) во векторную базу данных,
а затем отвечает на вопросы пользователей, комбинируя поиск релевантных фрагментов с генерацией ответа с помощью LLM.

## Цель проекта

* Автоматизировать обработку и поиск информации в текстовых датасетах (например, Wikipedia-параграфы).
* Обеспечить возможность быстрых ответов на вопросы пользователей без предварительного ручного анализа данных.
* Создать расширяемый прототип, который можно масштабировать под промышленные нагрузки.

---

## Архитектура

```
 ┌────────┐          ┌────────────────────┐          ┌───────────┐
 │  User  │ ───────▶│   FastAPI service   │ ───────▶│  LLM API  │
 └────────┘   ask    │ (поиск + генерация)│          └───────────┘
                     │   ▲
                     │   │
                     │   │      ┌───────────┐
                     └───│─────▶│ Chroma DB │
                         │      └───────────┘
                         │
                Indexing service
```

**Компоненты:**

* **Indexing service** – выполняет препроцессинг документов (очистку, фильтрацию, удаление дублей), создает эмбеддинги и сохраняет их в ChromaDB.
* **FastAPI service** – REST API, который принимает вопросы, ищет релевантные параграфы в базе и передает их вместе с вопросом в LLM (OpenAI, Anthropic или Google) для генерации ответа.

---

## Установка зависимостей

Убедитесь, что установлен Python 3.10+.

```bash
pip install -r requirements.txt
```

Создайте файл `configs/.env` со своими API-ключами:

```dotenv
DEBUG=False
OPENAI_API_KEY=your_api_key
ANTHROPIC_API_KEY=your_api_key
GOOGLE_API_KEY=your_api_key
```

---

## Запуск

### 1. Локально

1. Выполните индексацию данных (например, RuBQ 2.0) с помощью эндпоинта `/index_text` или `/index_file`.
2. Запустите API:

   ```bash
   uvicorn src.api.main:app --reload
   ```
3. Перейдите на `http://localhost:8000/static` для работы с веб-формой или используйте API-эндпоинты:

   * `/index_text` — индексация текстов (список документов).
   * `/index_file` — индексация данных из JSON-файла.
   * `/query` — получение ответа на вопрос.

### 2. Через Docker

```bash
docker-compose up --build
```

---

## Используемые технологии и обоснование

* **SentenceTransformers (`ai-forever/sbert_large_mt_nlu_ru`)** – модель для генерации русскоязычных эмбеддингов (подходит для семантического поиска).
* **ChromaDB** – лёгкая и быстрая векторная база данных, удобная для прототипов. В перспективе может быть заменена на Qdrant или Weaviate.
* **FastAPI** – асинхронный и производительный REST API-фреймворк.
* **LangChain** – интеграция с LLM API (OpenAI GPT-4, Anthropic Claude, Google Gemini).
* **Loguru** – удобное логирование с ротацией логов.

---

## API эндпоинты

1. **`POST /index_text`**
   Индексация списка документов:

   ```json
   [
     {"uid": "1", "text": "Первый параграф"},
     {"uid": "2", "text": "Второй параграф"}
   ]
   ```

   Ответ:

   ```json
   {"added": ["1", "2"], "message": "Добавлено 2 документов"}
   ```

2. **`POST /index_file`**
   Индексация документов из загружаемого JSON-файла.

3. **`POST /query`**
   Получение ответа на вопрос:

   ```json
   {"question": "Кто был первым президентом России?"}
   ```

   Ответ:

   ```json
   {"answer": "Борис Ельцин."}
   ```

---

## Анализ датасета RuBQ\_2.0

**Поля датасета:**

* **`uid`** — уникальный идентификатор записи.
* **`ru_wiki_pageid`** — идентификатор страницы в русской Википедии.
* **`text`** — текст параграфа (основное содержимое для индексации и поиска).

**Общая информация:**

* Всего строк в датасете: **56,952**
* Уникальные параграфы (по тексту): **56,826**
* Дубликаты (по ID и тексту): **отсутствуют**
* Пустые параграфы: **0**
* Короткие параграфы (<=20 символов): **315**
* Битые символы: **не обнаружены**

### Статистика длины параграфов (в символах)

* Средняя длина: **\~449**
* Минимальная длина: **1**
* Максимальная длина: **11,010**
* Медиана (50%): **343**
* 25% квантиль (Q1): **172**
* 75% квантиль (Q3): **600**
* 90% квантиль: **924**
* 95% квантиль: **1,173**

**Вывод:**
Основная масса параграфов (50%) имеет длину от **172 до 600 символов**.
Есть редкие длинные параграфы (хвосты) длиной до **11,000 символов**, но их немного.
Подробнее в [`data/eda.ipynb`](data/eda.ipynb).

---

## Проверка качества данных

В пайплайне используется класс `Preprocessor`, который:

* проверяет структуру (`uid`, `text`) и исключает битые или пустые документы;
* удаляет дубликаты (по `uid` и хешу текста);
* очищает текст от HTML, спецсимволов и множественных пробелов;
* фильтрует слишком короткие параграфы (менее 20 символов).

Все исключённые документы логируются для анализа.

---

## Тестирование

Запуск тестов из корневой директории:

```bash
python -m pytest tests/
```

---

## Потенциальные пути масштабирования

* Замена ChromaDB на более производительную распределённую базу (Qdrant, Weaviate). Или возможность по конфигу определять развертываемую БД.
* Вынесение индексации в отдельный сервис с очередями задач.
* Добавление кэширования результатов для популярных запросов.
* Добавление развертывания своей открытой модели. (Например, с HF)
* Ускорение кода с помощью multiprocessing.
* Добавление функционала загрузки документа любого типа и приведения к нужному формату.
* Дополнение АПИ эндпоинтами для очистки векторной БД.
* Ролевой доступ к разным типам данных из БД.
